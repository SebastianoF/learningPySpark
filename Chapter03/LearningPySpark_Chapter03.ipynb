{"cells":[{"cell_type":"markdown","source":["# Learning PySpark\n","## Chapter 3: DataFrames\n","\n","This notebook contains sample code from Chapter 3 of [Learning PySpark]() focusing on PySpark and DataFrames."],"metadata":{}},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import pyspark\n","\n","\n","sc = pyspark.SparkContext(\"local\", \"AppCh3\")\n","sqlContext = pyspark.SQLContext(sc)"]},{"cell_type":"markdown","source":["### Generate your own DataFrame\nInstead of accessing the file system, let's create a DataFrame by generating the data.  In this case, we'll first create the `stringRDD` RDD and then convert it into a DataFrame when we're reading `stringJSONRDD` using `spark.read.json`."],"metadata":{}},{"cell_type":"code","source":["# Generate our own JSON data \n","#   This way we don't have to access the file system yet.\n","stringJSONRDD = sc.parallelize((\"\"\" \n","  { \"id\": \"123\",\n","    \"name\": \"Katie\",\n","    \"age\": 19,\n","    \"eyeColor\": \"brown\"\n","  }\"\"\",\n","   \"\"\"{\n","    \"id\": \"234\",\n","    \"name\": \"Michael\",\n","    \"age\": 22,\n","    \"eyeColor\": \"green\"\n","  }\"\"\", \n","  \"\"\"{\n","    \"id\": \"345\",\n","    \"name\": \"Simone\",\n","    \"age\": 23,\n","    \"eyeColor\": \"blue\"\n","  }\"\"\")\n",")"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["# Create DataFrame\n","swimmersJSON = sqlContext.read.json(stringJSONRDD)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# Create temporary table\nswimmersJSON.createOrReplaceTempView(\"swimmersJSON\")"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# DataFrame API\n","print(type(swimmersJSON))\n","swimmersJSON.show()"],"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":"<class 'pyspark.sql.dataframe.DataFrame'>\n+---+--------+---+-------+\n|age|eyeColor| id|   name|\n+---+--------+---+-------+\n| 19|   brown|123|  Katie|\n| 22|   green|234|Michael|\n| 23|    blue|345| Simone|\n+---+--------+---+-------+\n\n"}],"execution_count":5},{"cell_type":"markdown","metadata":{},"source":["#### Pandas DataFrames and pyspark dataFrames\n","\n","The dataframe can be converted to a pandas dataframe easily:"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["df = swimmersJSON.toPandas()"]},{"cell_type":"markdown","metadata":{},"source":["... and backwards."]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":"+---+--------+---+-------+\n|age|eyeColor| id|   name|\n+---+--------+---+-------+\n| 19|   brown|123|  Katie|\n| 22|   green|234|Michael|\n| 23|    blue|345| Simone|\n+---+--------+---+-------+\n\n"}],"source":["swimmersJSON_from_pandas = sqlContext.createDataFrame(df)\n","swimmersJSON_from_pandas.show()"]},{"cell_type":"markdown","metadata":{},"source":["#### Sql query:"]},{"cell_type":"code","source":["sqlContext.sql(\"select * from swimmersJSON\").collect()"],"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"[Row(age=19, eyeColor='brown', id='123', name='Katie'),\n Row(age=22, eyeColor='green', id='234', name='Michael'),\n Row(age=23, eyeColor='blue', id='345', name='Simone')]"},"metadata":{},"execution_count":8}],"execution_count":8},{"cell_type":"markdown","source":["#### Inferring the Schema Using Reflection\nNote that Apache Spark is inferring the schema using reflection; i.e. it automaticlaly determines the schema of the data based on reviewing the JSON data."],"metadata":{}},{"cell_type":"code","source":["# Print the schema\nswimmersJSON.printSchema()"],"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":"root\n |-- age: long (nullable = true)\n |-- eyeColor: string (nullable = true)\n |-- id: string (nullable = true)\n |-- name: string (nullable = true)\n\n"}],"execution_count":11},{"cell_type":"markdown","source":["Notice that Spark was able to determine infer the schema (when reviewing the schema using `.printSchema`).\n\nBut what if we want to programmatically specify the schema?"],"metadata":{}},{"cell_type":"markdown","source":["#### Programmatically Specifying the Schema\nIn this case, let's specify the schema for a `CSV` text file."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import types\n","\n","# Generate our own CSV data \n","#   This way we don't have to access the file system yet.\n","stringCSVRDD = sc.parallelize(\n","    [\n","        (123, 'Katie', 19, 'brown'), \n","        (234, 'Michael', 22, 'green'), \n","        (345, 'Simone', 23, 'blue')\n","    ]\n",")\n","\n","# The schema is encoded in a string, using StructType we define the schema using various pyspark.sql.types\n","schemaString = \"id name age eyeColor\"\n","schema = types.StructType(\n","    [\n","        types.StructField(\"id\",       types.LongType(),   True),    \n","        types.StructField(\"name\",     types.StringType(), True),\n","        types.StructField(\"age\",      types.LongType(),   True),\n","        types.StructField(\"eyeColor\", types.StringType(), True)\n","    ]\n",")\n","\n","# Apply the schema to the RDD and Create DataFrame\n","swimmers = sqlContext.createDataFrame(stringCSVRDD, schema)\n","\n","# Creates a temporary view using the DataFrame\n","swimmers.createOrReplaceTempView(\"swimmers\")"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# Print the schema\n","#   Notice that we have redefined id as Long (instead of String)\n","print(type(swimmers))\n","swimmers.printSchema()"],"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":"<class 'pyspark.sql.dataframe.DataFrame'>\nroot\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- age: long (nullable = true)\n |-- eyeColor: string (nullable = true)\n\n"}],"execution_count":13},{"cell_type":"markdown","source":["As you can see from above, we can programmatically apply the `schema` instead of allowing the Spark engine to infer the schema via reflection.\n\nAdditional Resources include:\n* [PySpark API Reference](https://spark.apache.org/docs/2.0.0/api/python/pyspark.sql.html)\n* [Spark SQL, DataFrames, and Datasets Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html#programmatically-specifying-the-schema): This is in reference to Programmatically Specifying the Schema using a `CSV` file."],"metadata":{}},{"cell_type":"markdown","metadata":{},"source":["## Intedlude: the spark UI\n","\n","Browsing the spark UI should give an idea how can `sqlContext.sql(\"select * from swimmersJSON\").collect()` work from the jupyter notebook, even if swimmerJSON is passed as a string and not as a variable."]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":"http://localhost:4040/jobs/\n"}],"source":["spark_host = \"localhost\"\n","local_port = sc.uiWebUrl.split(':')[-1]\n","\n","spark_UI_url = f\"http://{spark_host}:{local_port}/jobs/\"\n","\n","print(spark_UI_url)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{},"execution_count":16}],"source":["import webbrowser\n","\n","webbrowser.open(spark_UI_url)"]},{"cell_type":"markdown","source":["## SparkSession: queries via sql and API\n","\n","* Entry point for reading data\n","* Working with metadata\n","* Configuration\n","* Cluster resource management\n","\n","For more information, please refer to [How to use SparkSession in Apache Spark 2.0](http://bit.ly/2br0Fr1) (http://bit.ly/2br0Fr1)."],"metadata":{}},{"cell_type":"markdown","source":["Get the table names and the elements in swimmers"],"metadata":{}},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":"['swimmers', 'swimmersjson']\n"},{"output_type":"execute_result","data":{"text/plain":"3"},"metadata":{},"execution_count":62}],"source":["print(sqlContext.tableNames())\n","swimmers.count()"]},{"cell_type":"markdown","metadata":{},"source":["With DataFrames, you can start writing your queries using `Spark SQL` - a SQL dialect that is compatible with the Hive Query Language (or HiveQL).\n","\n","Execute an sql query and return the data:"]},{"cell_type":"code","source":["sqlContext.sql(\"select * from swimmers\").show()"],"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":"+---+-------+---+--------+\n| id|   name|age|eyeColor|\n+---+-------+---+--------+\n|123|  Katie| 19|   brown|\n|234|Michael| 22|   green|\n|345| Simone| 23|    blue|\n+---+-------+---+--------+\n\n"}],"execution_count":44},{"cell_type":"markdown","metadata":{},"source":["... and convert to a Pandas dataframe"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"    id     name  age eyeColor\n0  123    Katie   19    brown\n1  234  Michael   22    green\n2  345   Simone   23     blue","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>name</th>\n      <th>age</th>\n      <th>eyeColor</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>123</td>\n      <td>Katie</td>\n      <td>19</td>\n      <td>brown</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>234</td>\n      <td>Michael</td>\n      <td>22</td>\n      <td>green</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>345</td>\n      <td>Simone</td>\n      <td>23</td>\n      <td>blue</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":50}],"source":["sqlContext.sql(\"select * from swimmers\").toPandas()"]},{"cell_type":"markdown","source":["Let's get the row count:"],"metadata":{}},{"cell_type":"code","source":["sqlContext.sql(\"select count(1) from swimmers\").show()"],"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":"+--------+\n|count(1)|\n+--------+\n|       3|\n+--------+\n\n"}],"execution_count":52},{"cell_type":"markdown","metadata":{},"source":["Query id and age for swimmers with age = 22 via sql and API"]},{"cell_type":"code","source":["sqlContext.sql(\"select id, age from swimmers where age = 22\").show()"],"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":"+---+---+\n| id|age|\n+---+---+\n|234| 22|\n+---+---+\n\n"}],"execution_count":22},{"cell_type":"code","source":["swimmers.select(\"id\", \"age\").filter(\"age = 22\").show()"],"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":"+---+---+\n| id|age|\n+---+---+\n|234| 22|\n+---+---+\n\n"}],"execution_count":20},{"cell_type":"code","source":["swimmers.select(swimmers.id, swimmers.age).filter(swimmers.age == 22).show()"],"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":"+---+---+\n| id|age|\n+---+---+\n|234| 22|\n+---+---+\n\n"}],"execution_count":53},{"cell_type":"markdown","metadata":{},"source":["**Exercise:** transform the following sql query in an API query"]},{"cell_type":"code","source":["sqlContext.sql(\"select name, eyeColor from swimmers where eyeColor like 'b%'\").show()"],"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":"+------+--------+\n|  name|eyeColor|\n+------+--------+\n| Katie|   brown|\n|Simone|    blue|\n+------+--------+\n\n"}],"execution_count":58},{"cell_type":"markdown","source":["# On-Time Flight Performance Showcase\n","\n","We analyse the *Airline On-time Performance and Causes of Flights dealy*, based on the followdig datasets:\n","\n","+ https://catalog.data.gov/dataset/airline-on-time-performance-and-causes-of-flight-delays-on-time-data  (from the book - link out of date)\n","\n","+ https://openflights.org/data.html\n","\n","And copyied locally under `./Data`."],"metadata":{}},{"cell_type":"markdown","source":["### DataFrame Queries\nLet's run a flight performance using DataFrames; let's first build the DataFrames from the source datasets."],"metadata":{}},{"cell_type":"code","source":["data_folder = os.path.join(os.path.dirname(os.getcwd()), \"Data\")\n","\n","# Set File Paths\n","flightPerfFilePath = os.path.join(data_folder, \"departuredelays.csv\")\n","airportsFilePath = os.path.join(data_folder, \"airport-codes-na.txt\")\n","\n","for f in [flightPerfFilePath, airportsFilePath]:\n","    assert os.path.exists(f), f\"File {f} not found.\"\n","\n","# Obtain Airports dataset\n","airports = sqlContext.read.csv(airportsFilePath, header='true', inferSchema='true', sep='\\t')\n","airports.createOrReplaceTempView(\"airports\")\n","\n","# Obtain Departure Delays dataset\n","flightPerf = sqlContext.read.csv(flightPerfFilePath, header='true')\n","flightPerf.createOrReplaceTempView(\"FlightPerformance\")\n","\n","# Cache the Departure Delays dataset \n","flightPerf.cache()"],"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"DataFrame[date: string, delay: string, distance: string, origin: string, destination: string]"},"metadata":{},"execution_count":64}],"execution_count":64},{"cell_type":"markdown","metadata":{},"source":["List the tables in the current context and list the number of elements:"]},{"cell_type":"code","execution_count":70,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":"['airports', 'flightperformance', 'swimmers', 'swimmersjson']\n526\n"},{"output_type":"execute_result","data":{"text/plain":"1391578"},"metadata":{},"execution_count":70}],"source":["print(sqlContext.tableNames())\n","print(airports.count())\n","flightPerf.count()"]},{"cell_type":"markdown","metadata":{},"source":["Query Sum of Flight Delays by City and Origin Code (for Washington State).\n","\n","Equivalent sql code is:\n","\n","```sql\n","select a.City, f.origin, sum(f.delay) as Delays\n","  from FlightPerformance f\n","    join airports a\n","      on a.IATA = f.origin\n"," where a.State = 'WA'\n"," group by a.City, f.origin\n"," order by sum(f.delay) desc\n","```\n"]},{"cell_type":"code","source":["sqlContext.sql(\"select a.City, f.origin, sum(f.delay) as Delays from FlightPerformance f join airports a on a.IATA = f.origin where a.State = 'WA' group by a.City, f.origin order by sum(f.delay) desc\").show()"],"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":"+-------+------+--------+\n|   City|origin|  Delays|\n+-------+------+--------+\n|Seattle|   SEA|159086.0|\n|Spokane|   GEG| 12404.0|\n|  Pasco|   PSC|   949.0|\n+-------+------+--------+\n\n"}],"execution_count":71},{"cell_type":"markdown","metadata":{},"source":["Query Sum of Flight Delays by State (for the US)\n","\n","```sql\n","select a.State, sum(f.delay) as Delays\n","  from FlightPerformance f\n","    join airports a\n","      on a.IATA = f.origin\n"," where a.Country = 'USA'\n"," group by a.State \n","```"]},{"cell_type":"code","source":["sqlContext.sql(\"select a.State, sum(f.delay) as Delays from FlightPerformance f join airports a on a.IATA = f.origin where a.Country = 'USA' group by a.State \").show()"],"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":"+-----+---------+\n|State|   Delays|\n+-----+---------+\n|   SC|  80666.0|\n|   AZ| 401793.0|\n|   LA| 199136.0|\n|   MN| 256811.0|\n|   NJ| 452791.0|\n|   OR| 109333.0|\n|   VA|  98016.0|\n| null| 397237.0|\n|   RI|  30760.0|\n|   WY|  15365.0|\n|   KY|  61156.0|\n|   NH|  20474.0|\n|   MI| 366486.0|\n|   NV| 474208.0|\n|   WI| 152311.0|\n|   ID|  22932.0|\n|   CA|1891919.0|\n|   CT|  54662.0|\n|   NE|  59376.0|\n|   MT|  19271.0|\n+-----+---------+\nonly showing top 20 rows\n\n"}],"execution_count":73},{"cell_type":"markdown","metadata":{},"source":["**Exercise:** rewrite the queries above from sql to API."]},{"cell_type":"markdown","metadata":{},"source":["## Data visualisation:\n","\n","You can visualize the data using databricks (the query above in the databrics environment), or you can use some python libraries."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Barchart TODO"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# map TODO"]},{"cell_type":"code","execution_count":74,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"['date', 'delay', 'distance', 'origin', 'destination']"},"metadata":{},"execution_count":74}],"source":["flightPerf.columns"]},{"cell_type":"markdown","metadata":{},"source":["** Exercise:** from the dataset alone it is not possible to get the number of total fligths in the queried states, therefore we can not normalise and show the delay rate.\n","Finding the dataset and normalising the data is left as an exercise"]},{"cell_type":"markdown","source":["For more information, please refer to:\n* [Spark SQL, DataFrames and Datasets Guide](http://spark.apache.org/docs/latest/sql-programming-guide.html#sql)\n* [PySpark SQL Module: DataFrame](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame)\n* [PySpark SQL Functions Module](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions)"],"metadata":{}},{"cell_type":"markdown","metadata":{},"source":["## Chapter's Summary:\n","\n","+ Craeting Dataframes\n","    + Generating a JSON table\n","    + `sqlContext`\n","    + Creating a temporary view\n","    + Interlude: open the Spark UI\n","+ Simple DataFrame queries\n","    + Via sql or API\n","        + .show\n","        + .sql\n","        + .select\n","        + .filte\n","+ Case study: delayed flights"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"name":"Ch4 - DataFrames","notebookId":4341522646494009,"kernelspec":{"name":"python37564bitvenvvirtualenv746bf185f140465ab8609cf377657fb8","display_name":"Python 3.7.5 64-bit ('venv': virtualenv)"}},"nbformat":4,"nbformat_minor":0}