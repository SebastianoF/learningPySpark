{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning PySpark\n",
    "\n",
    "## Chapter 4: Prepare and understand data for modeling\n",
    "\n",
    "This notebook contains sample code from Chapter 3 of Learning PySpark focusing on PySpark and DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "\n",
    "sc = pyspark.SparkContext(\"local\", \"AppCh3\")\n",
    "sqlContext = pyspark.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicates\n",
    "\n",
    "Consider the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---+------+------+---+------+\n| id|weight|height|age|gender|\n+---+------+------+---+------+\n|  1| 144.5|   5.9| 33|     M|\n|  2| 167.2|   5.4| 45|     M|\n|  3| 124.1|   5.2| 23|     F|\n|  4| 144.5|   5.9| 33|     M|\n|  5| 133.2|   5.7| 54|     F|\n|  3| 124.1|   5.2| 23|     F|\n|  5| 129.2|   5.3| 42|     M|\n+---+------+------+---+------+\n\n"
    }
   ],
   "source": [
    "df = sqlContext.createDataFrame(\n",
    "    [\n",
    "        (1, 144.5, 5.9, 33, 'M'),\n",
    "        (2, 167.2, 5.4, 45, 'M'),\n",
    "        (3, 124.1, 5.2, 23, 'F'),\n",
    "        (4, 144.5, 5.9, 33, 'M'),\n",
    "        (5, 133.2, 5.7, 54, 'F'),\n",
    "        (3, 124.1, 5.2, 23, 'F'),\n",
    "        (5, 129.2, 5.3, 42, 'M'),\n",
    "    ], \n",
    "    [\n",
    "        'id',\n",
    "        'weight',\n",
    "        'height',\n",
    "        'age',\n",
    "        'gender'\n",
    "    ]\n",
    ")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Count of rows: 7\nCount of distinct rows: 6\n"
    }
   ],
   "source": [
    "print('Count of rows: {0}'.format(df.count()))\n",
    "print('Count of distinct rows: {0}'.format(df.distinct().count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If these two numbers differ - you have rows that are exact copies of each other. We can drop these rows by using the `.dropDuplicates(...)` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---+------+------+---+------+\n| id|weight|height|age|gender|\n+---+------+------+---+------+\n|  5| 133.2|   5.7| 54|     F|\n|  5| 129.2|   5.3| 42|     M|\n|  1| 144.5|   5.9| 33|     M|\n|  4| 144.5|   5.9| 33|     M|\n|  2| 167.2|   5.4| 45|     M|\n|  3| 124.1|   5.2| 23|     F|\n+---+------+------+---+------+\n\n"
    }
   ],
   "source": [
    "df = df.dropDuplicates()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Count of rows: 6\nCount of distinct ids: 5\n"
    }
   ],
   "source": [
    "print('Count of rows: {0}'.format(df.count()))\n",
    "print('Count of distinct ids: {0}'.format(df.select([c for c in df.columns if c != 'id']).distinct().count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have one more duplicate. We will use the `.dropDuplicates(...)` but add the `subset` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---+------+------+---+------+\n| id|weight|height|age|gender|\n+---+------+------+---+------+\n|  5| 133.2|   5.7| 54|     F|\n|  1| 144.5|   5.9| 33|     M|\n|  2| 167.2|   5.4| 45|     M|\n|  3| 124.1|   5.2| 23|     F|\n|  5| 129.2|   5.3| 42|     M|\n+---+------+------+---+------+\n\n"
    }
   ],
   "source": [
    "df = df.dropDuplicates(subset=[c for c in df.columns if c != 'id'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort the by the first column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---+------+------+---+------+\n| id|weight|height|age|gender|\n+---+------+------+---+------+\n|  1| 144.5|   5.9| 33|     M|\n|  2| 167.2|   5.4| 45|     M|\n|  3| 124.1|   5.2| 23|     F|\n|  5| 133.2|   5.7| 54|     F|\n|  5| 129.2|   5.3| 42|     M|\n+---+------+------+---+------+\n\n"
    }
   ],
   "source": [
    "df = df.sort('id')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the total and distinct number of IDs in one step we can use the `.agg(...)` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+-----+--------+\n|count|distinct|\n+-----+--------+\n|    5|       4|\n+-----+--------+\n\n"
    }
   ],
   "source": [
    "import pyspark.sql.functions as fn\n",
    "\n",
    "df.agg(\n",
    "    fn.count('id').alias('count'),\n",
    "    fn.countDistinct('id').alias('distinct')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give each row a unique ID. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---+------+------+---+------+-----------+\n| id|weight|height|age|gender|     new_id|\n+---+------+------+---+------+-----------+\n|  1| 144.5|   5.9| 33|     M|          0|\n|  2| 167.2|   5.4| 45|     M| 8589934592|\n|  3| 124.1|   5.2| 23|     F|17179869184|\n|  5| 133.2|   5.7| 54|     F|25769803776|\n|  5| 129.2|   5.3| 42|     M|25769803777|\n+---+------+------+---+------+-----------+\n\n"
    }
   ],
   "source": [
    "df.withColumn('new_id', fn.monotonically_increasing_id()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing observations\n",
    "\n",
    "Consider a similar example to the one we presented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_miss = sqlContext.createDataFrame(\n",
    "    [\n",
    "        (1, 143.5, 5.6, 28,   'M',  100000),\n",
    "        (2, 167.2, 5.4, 45,   'M',  None),\n",
    "        (3, None , 5.2, None, None, None),\n",
    "        (4, 144.5, 5.9, 33,   'M',  None),\n",
    "        (5, 133.2, 5.7, 54,   'F',  None),\n",
    "        (6, 124.1, 5.2, None, 'F',  None),\n",
    "        (7, 129.2, 5.3, 42,   'M',  76000),\n",
    "    ], \n",
    "    [\n",
    "        'id', \n",
    "        'weight', \n",
    "        'height', \n",
    "        'age', \n",
    "        'gender', \n",
    "        'income'\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the number of missing observations per row we can use the following snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[(1, 0), (2, 1), (3, 4), (4, 1), (5, 1), (6, 2), (7, 0)]"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "df_miss.rdd.map(\n",
    "    lambda row: (row['id'], sum([c == None for c in row]))\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what values are missing so when we count missing observations in columns we can decide whether to drop the observation altogether or impute some of the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---+------+------+----+------+------+\n| id|weight|height| age|gender|income|\n+---+------+------+----+------+------+\n|  3|  null|   5.2|null|  null|  null|\n+---+------+------+----+------+------+\n\n"
    }
   ],
   "source": [
    "df_miss.where('id == 3').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the percentage of missing observations we see in each column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+----------+------------------+--------------+------------------+------------------+------------------+\n|id_missing|    weight_missing|height_missing|       age_missing|    gender_missing|    income_missing|\n+----------+------------------+--------------+------------------+------------------+------------------+\n|       0.0|0.1428571428571429|           0.0|0.2857142857142857|0.1428571428571429|0.7142857142857143|\n+----------+------------------+--------------+------------------+------------------+------------------+\n\n"
    }
   ],
   "source": [
    "df_miss.agg(*[\n",
    "    (1 - (fn.count(c) / fn.count('*'))).alias(c + '_missing')\n",
    "    for c in df_miss.columns\n",
    "]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will drop the `'income'` feature as most of its values are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---+------+------+----+------+\n| id|weight|height| age|gender|\n+---+------+------+----+------+\n|  1| 143.5|   5.6|  28|     M|\n|  2| 167.2|   5.4|  45|     M|\n|  3|  null|   5.2|null|  null|\n|  4| 144.5|   5.9|  33|     M|\n|  5| 133.2|   5.7|  54|     F|\n|  6| 124.1|   5.2|null|     F|\n|  7| 129.2|   5.3|  42|     M|\n+---+------+------+----+------+\n\n"
    }
   ],
   "source": [
    "df_miss_no_income = df_miss.select([c for c in df_miss.columns if c != 'income'])\n",
    "df_miss_no_income.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To drop the observations instead you can use the `.dropna(...)` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---+------+------+----+------+\n| id|weight|height| age|gender|\n+---+------+------+----+------+\n|  1| 143.5|   5.6|  28|     M|\n|  2| 167.2|   5.4|  45|     M|\n|  4| 144.5|   5.9|  33|     M|\n|  5| 133.2|   5.7|  54|     F|\n|  6| 124.1|   5.2|null|     F|\n|  7| 129.2|   5.3|  42|     M|\n+---+------+------+----+------+\n\n"
    }
   ],
   "source": [
    "df_miss_no_income.dropna(thresh=3).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To impute a mean, median or other *calculated* value you need to first calculate the value, create a dict with such values, and then pass it to the `.fillna(...)` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---+------------------+------+---+-------+\n| id|            weight|height|age| gender|\n+---+------------------+------+---+-------+\n|  1|             143.5|   5.6| 28|      M|\n|  2|             167.2|   5.4| 45|      M|\n|  3|140.28333333333333|   5.2| 40|missing|\n|  4|             144.5|   5.9| 33|      M|\n|  5|             133.2|   5.7| 54|      F|\n|  6|             124.1|   5.2| 40|      F|\n|  7|             129.2|   5.3| 42|      M|\n+---+------------------+------+---+-------+\n\n"
    }
   ],
   "source": [
    "means = df_miss_no_income.agg(\n",
    "    *[fn.mean(c).alias(c) for c in df_miss_no_income.columns if c != 'gender']\n",
    ").toPandas().to_dict('records')[0]\n",
    "\n",
    "means['gender'] = 'missing'\n",
    "\n",
    "df_miss_no_income.fillna(means).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers\n",
    "\n",
    "Consider another simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_outliers = sqlContext.createDataFrame(\n",
    "    [\n",
    "        (1, 143.5, 5.3, 28),\n",
    "        (2, 154.2, 5.5, 45),\n",
    "        (3, 342.3, 5.1, 99),\n",
    "        (4, 144.5, 5.5, 33),\n",
    "        (5, 133.2, 5.4, 54),\n",
    "        (6, 124.1, 5.1, 21),\n",
    "        (7, 129.2, 5.3, 42),\n",
    "    ], [\n",
    "        'id', \n",
    "        'weight', \n",
    "        'height',\n",
    "        'age'\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we calculate the lower and upper *cut off* points for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols = ['weight', 'height', 'age']\n",
    "bounds = {}\n",
    "\n",
    "for col in cols:\n",
    "    quantiles = df_outliers.approxQuantile(col, [0.25, 0.75], 0.05)\n",
    "    IQR = quantiles[1] - quantiles[0]\n",
    "    bounds[col] = [quantiles[0] - 1.5 * IQR, quantiles[1] + 1.5 * IQR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `bounds` dictionary holds the lower and upper bounds for each feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'weight': [91.69999999999999, 191.7],\n 'height': [4.499999999999999, 6.1000000000000005],\n 'age': [-11.0, 93.0]}"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now use it to flag our outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---+--------+--------+-----+\n| id|weight_o|height_o|age_o|\n+---+--------+--------+-----+\n|  1|   false|   false|false|\n|  2|   false|   false|false|\n|  3|    true|   false| true|\n|  4|   false|   false|false|\n|  5|   false|   false|false|\n|  6|   false|   false|false|\n|  7|   false|   false|false|\n+---+--------+--------+-----+\n\n"
    }
   ],
   "source": [
    "outliers = df_outliers.select(*['id'] + [\n",
    "    (\n",
    "        (df_outliers[c] < bounds[c][0]) | \n",
    "        (df_outliers[c] > bounds[c][1])\n",
    "    ).alias(c + '_o') for c in cols\n",
    "])\n",
    "outliers.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two outliers in the `weight` feature and two in the `age` feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---+------+\n| id|weight|\n+---+------+\n|  3| 342.3|\n+---+------+\n\n+---+---+\n| id|age|\n+---+---+\n|  3| 99|\n+---+---+\n\n"
    }
   ],
   "source": [
    "df_outliers = df_outliers.join(outliers, on='id')\n",
    "df_outliers.filter('weight_o').select('id', 'weight').show()\n",
    "df_outliers.filter('age_o').select('id', 'age').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand your data\n",
    "\n",
    "### Descriptive statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load our data and convert it to a Spark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.types as typ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we download the data and the we read them im."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "('/home/savoy/repos/learningPySpark/tmp/ccFraud.csv.gz',\n <http.client.HTTPMessage at 0x7f5edc6eba10>)"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "import os\n",
    "import urllib\n",
    "\n",
    "\n",
    "url = 'http://tomdrabas.com/data/LearningPySpark/ccFraud.csv.gz'\n",
    "\n",
    "\n",
    "data_folder = os.path.join(os.path.dirname(os.getcwd()), \"tmp\")\n",
    "\n",
    "data_filename = os.path.basename(url)\n",
    "data_path = os.path.join(data_folder, data_filename)\n",
    "\n",
    "if not os.path.isdir(data_folder):\n",
    "    os.mkdir(data_folder)\n",
    "\n",
    "# Download\n",
    "urllib.request.urlretrieve(url, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fraud = sc.textFile(data_path)\n",
    "header = fraud.first()\n",
    "\n",
    "fraud = fraud.filter(\n",
    "        lambda row: row != header\n",
    "    ).map(\n",
    "        lambda row: [int(elem) for elem in row.split(',')]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following, we create the schema for our `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fields = [\n",
    "    *[\n",
    "        typ.StructField(h[1:-1], typ.IntegerType(), True)\n",
    "        for h in header.split(',')\n",
    "    ]\n",
    "]\n",
    "\n",
    "schema = typ.StructType(fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create our `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fraud_df = sqlContext.createDataFrame(fraud, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the dataframe is ready we can calculate the basic descriptive statistics for our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "root\n |-- custID: integer (nullable = true)\n |-- gender: integer (nullable = true)\n |-- state: integer (nullable = true)\n |-- cardholder: integer (nullable = true)\n |-- balance: integer (nullable = true)\n |-- numTrans: integer (nullable = true)\n |-- numIntlTrans: integer (nullable = true)\n |-- creditLine: integer (nullable = true)\n |-- fraudRisk: integer (nullable = true)\n\n"
    }
   ],
   "source": [
    "fraud_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For categorical columns we will count the frequencies of their values using `.groupby(...)` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fraud_df_ = fraud_df.groupby('gender').count()  # beware may take a while\n",
    "fraud_df_.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the truly numerical features we can use the `.describe()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numerical = ['balance', 'numTrans', 'numIntlTrans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "desc = fraud_df.describe(numerical)\n",
    "desc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how you check skewness (we will do it for the `'balance'` feature only)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fraud_df.agg({'balance': 'skewness'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Calculating correlations in PySpark is very easy once your data is in a DataFrame form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fraud_df.corr('balance', 'numTrans')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create a correlations matrix you can use the script below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_numerical = len(numerical)\n",
    "\n",
    "corr = []\n",
    "\n",
    "for i in range(0, n_numerical):\n",
    "    temp = [None] * i\n",
    "    \n",
    "    for j in range(i, n_numerical):\n",
    "        temp.append(fraud_df.corr(numerical[i], numerical[j]))\n",
    "    corr.append(temp)\n",
    "    \n",
    "corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "First, let's load the modules and set them up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import bokeh.charts as chrt\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Histograms\n",
    "\n",
    "Aggreagate the data in workers and return aggregated list of cut-off points and counts in each bin of the histogram to the driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hists = fraud_df.select('balance').rdd.flatMap(lambda row: row).histogram(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot the histogram you can simply call the matplotlib like below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "    'bins': hists[0][:-1],\n",
    "    'freq': hists[1]\n",
    "}\n",
    "\n",
    "fig = plt.figure(figsize=(12,9))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.bar(data['bins'], data['freq'], width=2000)\n",
    "ax.set_title('Histogram of \\'balance\\'')\n",
    "\n",
    "plt.savefig('B05793_05_22.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a similar manner, a histogram can be create with Bokeh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b_hist = chrt.Bar(data, values='freq', label='bins', title='Histogram of \\'balance\\'')\n",
    "chrt.show(b_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your data is small enough to fit on the driver (although we would argue it would normally be faster to use the method showed above) you can bring the data and use the `.hist(...)` (from Matplotlib) or `.Histogram(...)` (from Bokeh) methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_driver = {'obs': fraud_df.select('balance').rdd.flatMap(lambda row: row).collect()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,9))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "ax.hist(data_driver['obs'], bins=20)\n",
    "ax.set_title('Histogram of \\'balance\\' using .hist()')\n",
    "\n",
    "\n",
    "plt.savefig('B05793_05_24.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b_hist_driver = chrt.Histogram(data_driver, values='obs', title='Histogram of \\'balance\\' using .Histogram()', bins=20)\n",
    "chrt.show(b_hist_driver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactions between features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we will sample our fraud dataset at 1% given gender as strata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_sample = fraud_df.sampleBy('gender', {1: 0.0002, 2: 0.0002}).select(numerical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To put multiple 2D charts in one go you can use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_multi = dict([\n",
    "    (elem, data_sample.select(elem).rdd.flatMap(lambda row: row).collect()) \n",
    "    for elem in numerical\n",
    "])\n",
    "\n",
    "sctr = chrt.Scatter(data_multi, x='balance', y='numTrans')\n",
    "\n",
    "chrt.show(sctr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter's Summary\n",
    "\n",
    "Exploratory data analysis in pyspark\n",
    "+ duplicates, missing observations and outliers\n",
    "   + exact, non-id duplicates, id duplicates\n",
    "   + count\n",
    "   + drop duplicates\n",
    "   + agg\n",
    "   + functions\n",
    "   + counts\n",
    "   + count distinct\n",
    "   + alias\n",
    "+ Missin observations\n",
    "  + where\n",
    "  + select\n",
    "  + dropna\n",
    "  + fillna\n",
    "+ outliers\n",
    "  + create bounds and filter outliers\n",
    "  + approx quartile\n",
    "  + join\n",
    "  + select\n",
    "+ Exploer your data\n",
    "  + first\n",
    "  + groupby\n",
    "  + describe\n",
    "  + aggregate functions\n",
    "  + correlations\n",
    "+ Visualisation\n",
    "  + histogram\n",
    "  + interactions between features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('venv': virtualenv)",
   "language": "python",
   "name": "python37564bitvenvvirtualenv746bf185f140465ab8609cf377657fb8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}